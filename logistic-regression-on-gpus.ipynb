{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":116062,"databundleVersionId":14875579,"sourceType":"competition"},{"sourceId":8236972,"sourceType":"datasetVersion","datasetId":4885707},{"sourceId":14053389,"sourceType":"datasetVersion","datasetId":8637089},{"sourceId":14263798,"sourceType":"datasetVersion","datasetId":9009330},{"sourceId":14267122,"sourceType":"datasetVersion","datasetId":9059976}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport polars as pl\nimport duckdb as dd\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport cv2\nimport pickle\nimport gc\nimport ctypes\nfrom pathlib import Path\nimport logging\nimport json\nimport multiprocessing as mp\nfrom concurrent.futures import ProcessPoolExecutor\nimport datetime\nfrom typing import Tuple, Optional\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport cuml\nimport cudf\nfrom cuml.linear_model import LogisticRegression as cuCML_LogisticRegression\nimport cupy as cp\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom cuml.model_selection import train_test_split\n# Optional: for evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom cuml.preprocessing import StandardScaler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-29T06:28:27.989405Z","iopub.execute_input":"2025-12-29T06:28:27.989929Z","iopub.status.idle":"2025-12-29T06:28:38.071222Z","shell.execute_reply.started":"2025-12-29T06:28:27.989904Z","shell.execute_reply":"2025-12-29T06:28:38.070594Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"print(f\"Polars version: {pl.__version__}\")\nprint(f\"CuPy version: {cp.__version__}\")\n\ntry:\n    #import cuml\n    print(f\"cuML version: {cuml.__version__}\")\n    print(f\"cuML version imported successfully.\")\n    \nexcept ImportError as e:\n    print(f\"cuML could not be imported. Ensure RAPIDS is installed correctly. Error: {e}\")\n    # If Cuml cannot be imported, the rest of the notebook will not work.\n    # In this case, it may make sense to stop Execution.\n    raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T06:28:41.856127Z","iopub.execute_input":"2025-12-29T06:28:41.856619Z","iopub.status.idle":"2025-12-29T06:28:41.861960Z","shell.execute_reply.started":"2025-12-29T06:28:41.856595Z","shell.execute_reply":"2025-12-29T06:28:41.861174Z"}},"outputs":[{"name":"stdout","text":"Polars version: 1.25.0\nCuPy version: 13.6.0\ncuML version: 25.02.01\ncuML version imported successfully.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"try:\n    gpu_array = cp.arange(10)\n    print(f\"CuPy array on this device: {gpu_array.device}\")\nexcept cp.cuda.runtime.CUDARuntimeError as e:\n    print(f\"CuPy device can't be started: {e}\")\n    print(\"Make sure your CUDA drivers and cuPy installation are correct.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Setup the test data on which predictions need to be generated","metadata":{}},{"cell_type":"code","source":"df_test = pl.read_parquet('/kaggle/input/cafa6-protein-go-terms-feat-labels/test_protein_features_esm2_480.parquet')\nprint(\"Shape of test features\", df_test.shape)\nprint(df_test.head(5))\n\nprots_for_submission = np.array(pl.Series(df_test.select(pl.col('protein_accession_id'))).to_list())\nprint(\"prots_for_submission -- \", prots_for_submission.shape)\n\nsubmission_embed_np_array = df_test['embedding_arrays'].to_numpy().astype(np.float32)\nprint(f\"NumPy array shape: {submission_embed_np_array.shape}\")\nprint(f\"NumPy array dtype: {submission_embed_np_array.dtype}\")\n\nsubmission_embeds_cp_array = cp.array(submission_embed_np_array)\nprint(f\"CuPy array shape: {submission_embeds_cp_array.shape}\")\nprint(f\"CuPy array dtype: {submission_embeds_cp_array.dtype}\")\nprint(f\"CuPy array device: {submission_embeds_cp_array.device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Run the training pipeline","metadata":{}},{"cell_type":"code","source":"features_path = '/kaggle/input/cafa6-protein-labels-features-depth-based-suman/train_protein_features_cc_6.parquet'\nlabels_path = '/kaggle/input/cafa6-protein-labels-features-depth-based-suman/train_protein_labels_cc_6.parquet'\nweights_path = '/kaggle/input/cafa-6-protein-function-prediction/IA.tsv'\ntraining_iterations = 2500\nc_value = 0.01\nproba_threshold = 0.75\nsubmission_filename = 'submission_df_cc_6.tsv'\n\ndef training_pipeline(features_path, labels_path, weights_path, training_iterations, c_value, proba_threshold, submission_filename):\n    df_train_features = pl.read_parquet(features_path)\n    print(\"Shape of training features\", df_train_features.shape)\n    print(df_train_features.head(5))\n\n    df_train_labels = pl.read_parquet(labels_path)\n    print(\"Shape of training labels\", df_train_labels.shape)\n    print(df_train_labels.head(5))\n\n    label_cols = list(filter(lambda x: x != 'protein_accession_id', df_train_labels.columns))\n    print(\"length of label_cols -- \", len(label_cols))\n\n    df_weights = pl.read_csv(weights_path, separator=\"\\t\")\n    df_weights.columns = ['go_term', 'ia']\n    print(\"Shape of IA data\", df_weights.shape)\n\n    df_weights_filtered = df_weights.filter(pl.col('go_term').is_in(label_cols))\n    print(\"shape of df_weights_filtered -- \", df_weights_filtered.shape)\n    \n    labels = df_weights_filtered.shape[0]\n    class_wt_dict = {}\n    \n    for i in range(labels):\n        class_wt_dict[df_weights_filtered.item(i,0)] = round(df_weights_filtered.item(i,1),5)\n\n    embed_np_array = df_train_features['protein_embedding'].to_numpy().astype(np.float32)\n    print(f\"NumPy array shape: {embed_np_array.shape}\")\n    print(f\"NumPy array dtype: {embed_np_array.dtype}\")\n\n    embeds_cp_array = cp.array(embed_np_array)\n    print(f\"CuPy array shape: {embeds_cp_array.shape}\")\n    print(f\"CuPy array dtype: {embeds_cp_array.dtype}\")\n    print(f\"CuPy array device: {embeds_cp_array.device}\")\n\n    go_terms_cp_array = cp.array(df_train_labels.select(label_cols).to_numpy())\n    valid_rows_mask = (go_terms_cp_array != 0).any(axis=1)\n    go_terms_cp_array_final = go_terms_cp_array[valid_rows_mask]\n    print(f\"CuPy array shape: {go_terms_cp_array_final.shape}\")\n    print(f\"CuPy array dtype: {go_terms_cp_array_final.dtype}\")\n    print(f\"CuPy array device: {go_terms_cp_array_final.device}\")\n\n    embeds_cp_array_final = embeds_cp_array[valid_rows_mask]\n    print(f\"CuPy array shape: {embeds_cp_array_final.shape}\")\n    print(f\"CuPy array dtype: {embeds_cp_array_final.dtype}\")\n    print(f\"CuPy array device: {embeds_cp_array_final.device}\")\n\n    scaler = StandardScaler()\n\n    embeds_cp_array_final_scaled = scaler.fit_transform(embeds_cp_array_final)\n    submission_embeds_cp_array_scaled = scaler.fit_transform(submission_embeds_cp_array)\n\n    print(\"standard scaling of embeddings completed ....\")\n\n    label_weight_vector = cp.array([class_wt_dict.get(goterm, 0) for goterm in label_cols])\n\n    # 2. Multiply the binary label matrix by the weight vector\n    # This gives a 1D weight for every sample based on its positive GO terms\n    sample_weights = go_terms_cp_array_final.dot(label_weight_vector)\n    \n    # 3. Normalize (Best practice: ensures the average weight is 1.0)\n    sample_weights = sample_weights / np.mean(sample_weights)\n\n    print(\"normalisation of weights completed .... \", sample_weights.shape)\n\n    base_model = cuCML_LogisticRegression(solver='qn', max_iter=training_iterations, output_type='numpy', C=c_value, fit_intercept=False)\n\n    multilabel_model = MultiOutputClassifier(base_model)\n    \n    print(\"Training model ...\")\n    multilabel_model.fit(embeds_cp_array_final_scaled.get(), go_terms_cp_array_final.get(), sample_weight=sample_weights.astype('float32'))\n    print(\"Training complete ...\")\n    \n    predictions_gpu = multilabel_model.predict_proba(submission_embeds_cp_array_scaled.get())\n    prob_positive = np.transpose([p[:, 1] for p in predictions_gpu])\n    print(\"predictions complete ... \", prob_positive.shape)\n\n    predictions_df_pl = pl.DataFrame(\n        prob_positive, # Ensure data is a numpy array\n        schema=label_cols # Assign the column names\n    )\n    \n    predictions_df_pl = predictions_df_pl.with_columns(\n        pl.Series(name=\"protein_accession_id\", values=prots_for_submission)\n    )\n    print(\"finished building the predictions_df_pl ... \", predictions_df_pl.shape)\n    \n    long_format_df = predictions_df_pl.melt(\n        id_vars=[\"protein_accession_id\"],          # Column to keep as identifier\n        value_vars=label_cols,                  # Columns to melt into rows\n        variable_name=\"go_term\",                   # Name for the column containing GO terms\n        value_name=\"probability\"                   # Name for the column containing scores\n    )\n    long_format_df = long_format_df.filter(pl.col(\"probability\") > proba_threshold)\n    print(\"finished building the submission format ... \", long_format_df.shape)\n    print(dd.sql(\"select count(distinct(protein_accession_id)), count(distinct(go_term)) from long_format_df\").pl())\n    \n    long_format_df.write_csv(submission_filename, separator=\"\\t\", include_header=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Combining all the submission datasets into one","metadata":{}},{"cell_type":"code","source":"submission_df_cc_concat = pl.read_csv('/kaggle/input/suman-cafa6-submission-df-all/submission_df_cc_concat.tsv', separator=\"\\t\")\nsubmission_df_cc_concat.columns = ['protein','GO_Term','Probability']\nprint(\"Shape - \", submission_df_cc_concat.shape)\n\nsubmission_df_cc_2 = pl.read_csv('/kaggle/input/suman-cafa6-submission-df-all/submission_df_cc_2.tsv', separator=\"\\t\")\nsubmission_df_cc_2.columns = ['protein','GO_Term','Probability']\nprint(\"Shape - \", submission_df_cc_2.shape)\n\nsubmission_df_cc_3 = pl.read_csv('/kaggle/input/suman-cafa6-submission-df-all/submission_df_cc_3.tsv', separator=\"\\t\")\nsubmission_df_cc_3.columns = ['protein','GO_Term','Probability']\nprint(\"Shape - \", submission_df_cc_3.shape)\n\nsubmission_df_cc_5 = pl.read_csv('/kaggle/input/suman-cafa6-submission-df-all/submission_df_cc_5.tsv', separator=\"\\t\")\nsubmission_df_cc_5.columns = ['protein','GO_Term','Probability']\nprint(\"Shape - \", submission_df_cc_5.shape)\n\nsubmission_df_cc_4 = pl.read_csv('/kaggle/input/suman-cafa6-submission-df-all/submission_df_cc_4.tsv', separator=\"\\t\")\nsubmission_df_cc_4.columns = ['protein','GO_Term','Probability']\nprint(\"Shape - \", submission_df_cc_4.shape)\n\nsubmission_df_cc_6 = pl.read_csv('/kaggle/input/suman-cafa6-submission-df-all/submission_df_cc_6.tsv', separator=\"\\t\")\nsubmission_df_cc_6.columns = ['protein','GO_Term','Probability']\nprint(\"Shape - \", submission_df_cc_6.shape)\n\nsubmission_df_cc = pl.concat([submission_df_cc_concat, submission_df_cc_6\n                          , submission_df_cc_4, submission_df_cc_5, submission_df_cc_3, submission_df_cc_2])\nprint(submission_df_cc.shape)\n\nunique_proteins_cc = submission_df_cc.select(\"protein\").unique()\n\nnew_rows_cc = unique_proteins_cc.with_columns(\n    GO_Term = pl.lit(\"GO:0005575\"),\n    Probability = pl.lit(0.999999)\n)\n\nsubmission_df_cc_w_root = pl.concat([submission_df_cc, new_rows_cc])\nprint(submission_df_cc_w_root.shape)\n\nsubmission_df_mf = pl.read_csv('/kaggle/input/suman-cafa6-submission-df-all/submission_df_mf.tsv', separator=\"\\t\")\nsubmission_df_mf.columns = ['protein','GO_Term','Probability']\nprint(\"Shape - \", submission_df_mf.shape)\n\nunique_proteins_mf = submission_df_mf.select(\"protein\").unique()\n\nnew_rows_mf = unique_proteins_mf.with_columns(\n    GO_Term = pl.lit(\"GO:0003674\"),\n    Probability = pl.lit(0.999999)\n)\n\nsubmission_df_mf_w_root = pl.concat([submission_df_mf, new_rows_mf])\nprint(submission_df_mf_w_root.shape)\n\nsubmission_df_bp = pl.read_csv('/kaggle/input/suman-cafa6-submission-df-all/submission_df_bp.tsv', separator=\"\\t\")\nsubmission_df_bp.columns = ['protein','GO_Term','Probability']\nprint(\"Shape - \", submission_df_bp.shape)\n\nunique_proteins_bp = submission_df_bp.select(\"protein\").unique()\n\nnew_rows_bp = unique_proteins_bp.with_columns(\n    GO_Term = pl.lit(\"GO:0008150\"),\n    Probability = pl.lit(0.999999)\n)\n\nsubmission_df_bp_w_root = pl.concat([submission_df_bp, new_rows_bp])\nprint(submission_df_bp_w_root.shape)\n\nsubmission_df = pl.concat([submission_df_mf_w_root, submission_df_cc_w_root, submission_df_bp_w_root])\nprint(submission_df.shape)\n\nprint(dd.sql(\"select count(distinct(protein)) as proteins, count(distinct(GO_Term)) as GO_Terms, from submission_df\").pl())\nsubmission_df.write_csv(\"submission.tsv\", separator=\"\\t\", include_header=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Miscellaneous operations","metadata":{}},{"cell_type":"code","source":"terms_with_max_wts = (\n    df_weights_filtered.top_k(35, by=\"ia\")\n    .get_column(\"go_term\")\n    .to_list()\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_cols_subset = label_cols[0:5]\nprint(len(label_cols_subset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:35:05.265801Z","iopub.execute_input":"2025-12-23T04:35:05.266595Z","iopub.status.idle":"2025-12-23T04:35:05.270537Z","shell.execute_reply.started":"2025-12-23T04:35:05.266567Z","shell.execute_reply":"2025-12-23T04:35:05.269798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"counts = cp.sum(go_terms_cp_array_final, axis=0)\n\n# Transfer only the small summary back to CPU for printing\nprint(\"Positive counts per label:\", counts.get().astype(int))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T07:33:39.026483Z","iopub.execute_input":"2025-12-23T07:33:39.027146Z","iopub.status.idle":"2025-12-23T07:33:39.140303Z","shell.execute_reply.started":"2025-12-23T07:33:39.027124Z","shell.execute_reply":"2025-12-23T07:33:39.139540Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"empty_check = (go_terms_cp_array_final.sum(axis=0) == 0).any()\nif empty_check:\n    print(\"Warning: One of the 5 selected labels has no positive samples in this subset.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T07:33:46.271604Z","iopub.execute_input":"2025-12-23T07:33:46.272313Z","iopub.status.idle":"2025-12-23T07:33:49.104670Z","shell.execute_reply.started":"2025-12-23T07:33:46.272287Z","shell.execute_reply":"2025-12-23T07:33:49.103887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"column_sums = np.sum(go_terms_cp_array_final, axis=1)\n\n# Identify indices of empty labels (where sum is zero)\nempty_indices = np.where(column_sums == 0)[0]\nlen(empty_indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:52:40.157577Z","iopub.execute_input":"2025-12-23T04:52:40.158063Z","iopub.status.idle":"2025-12-23T04:52:40.200389Z","shell.execute_reply.started":"2025-12-23T04:52:40.158040Z","shell.execute_reply":"2025-12-23T04:52:40.199621Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_gpu, X_test_gpu, y_train_gpu, y_test_gpu = train_test_split(\n    embeds_cp_array, go_terms_cp_array, test_size=0.1, random_state=42\n)\n\nprint(\"\\nShapes of train and test sets(GPU):\")\nprint(f\"X_train_gpu: {X_train_gpu.shape}\")\nprint(f\"X_test_gpu: {X_test_gpu.shape}\")\nprint(f\"y_train_gpu: {y_train_gpu.shape}\")\nprint(f\"y_test_gpu: {y_test_gpu.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T05:47:49.684130Z","iopub.execute_input":"2025-12-22T05:47:49.684848Z","iopub.status.idle":"2025-12-22T05:47:49.696635Z","shell.execute_reply.started":"2025-12-22T05:47:49.684825Z","shell.execute_reply":"2025-12-22T05:47:49.695909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import make_scorer, f1_score\n\nparam_grid = {\n    'estimator__C': [0.1, 1.0, 10.0],           # Regularization strength\n    'estimator__penalty': ['l2', 'none'],      # Penalty type\n    'estimator__tol': [1e-4, 1e-3]             # Tolerance for stopping\n}\n\n# Create a scorer that suppresses the warning by setting f1 to 0.0 for zero-division cases\nweighted_f1 = make_scorer(f1_score, average='weighted', zero_division=0)\n\n# 3. Initialize GridSearchCV\n# Use 'f1_macro' or 'f1_weighted' as GO terms are often highly imbalanced\ngrid_search = GridSearchCV(\n    multilabel_model, \n    param_grid, \n    cv=5, \n    scoring=weighted_f1, \n    n_jobs=1\n)\n\nprint(\"Training models (Grid Search)...\")\n# 4. Fit the grid search\n\ngrid_search.fit(embeds_cp_array_final.get(), go_terms_cp_array_final.get())\n\nprint(\"Training complete (Grid Search).\")\n# 5. Retrieve the best parameters\nprint(\"Optimal Hyperparameters:\", grid_search.best_params_)\n\npredictions_gpu = grid_search.predict_proba(submission_embeds_cp_array.get())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:55:34.695130Z","iopub.execute_input":"2025-12-23T04:55:34.695842Z","iopub.status.idle":"2025-12-23T04:55:40.204915Z","shell.execute_reply.started":"2025-12-23T04:55:34.695817Z","shell.execute_reply":"2025-12-23T04:55:40.203999Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"long_format_df.write_parquet(\"submission_df_cc_4.parquet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T14:08:24.426810Z","iopub.execute_input":"2025-12-25T14:08:24.427521Z","iopub.status.idle":"2025-12-25T14:08:48.198304Z","shell.execute_reply.started":"2025-12-25T14:08:24.427497Z","shell.execute_reply":"2025-12-25T14:08:48.197454Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"import gzip\n\nfile_path = 'submission_df_cc_4.tsv.gz'\n\n# Open the file using gzip.open in write-bytes mode ('wb')\nwith gzip.open(file_path, 'wb') as f:\n    # Write the DataFrame to the file object\n    long_format_df.write_csv(f, separator=\"\\t\", include_header=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T14:00:04.168457Z","iopub.execute_input":"2025-12-25T14:00:04.169150Z","iopub.status.idle":"2025-12-25T14:04:43.793970Z","shell.execute_reply.started":"2025-12-25T14:00:04.169102Z","shell.execute_reply":"2025-12-25T14:04:43.793025Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1566411420.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Write the DataFrame to the file object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mlong_format_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/polars/dataframe/frame.py\u001b[0m in \u001b[0;36mwrite_csv\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2969\u001b[0m             \u001b[0mstorage_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2971\u001b[0;31m         self._df.write_csv(\n\u001b[0m\u001b[1;32m   2972\u001b[0m             \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2973\u001b[0m             \u001b[0minclude_bom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: "],"ename":"OSError","evalue":"","output_type":"error"}],"execution_count":96}]}